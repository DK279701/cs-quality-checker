import streamlit as st
import pandas as pd
import requests
import aiohttp
import asyncio
import time
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

st.set_page_config(page_title="CS Quality Checker", layout="wide")
st.title("ðŸ“¥ Analiza wiadomoÅ›ci wyÅ‚Ä…cznie wybranych agentÃ³w (ostatnie 7 dni)")

# â€” Sidebar: klucze API â€”
st.sidebar.header("ðŸ”‘ Klucze API")
front_token = st.sidebar.text_input("Front API Token", type="password")
openai_key  = st.sidebar.text_input("OpenAI API Key",   type="password")
if not front_token or not openai_key:
    st.sidebar.warning("Podaj oba klucze API (Front i OpenAI).")
    st.stop()

# â€” StaÅ‚e inboxy â€”
INBOX_IDS = ["inb_a3xxy","inb_d2uom","inb_d2xee"]
st.sidebar.markdown("**Analizowane inboxy:**")
st.sidebar.write("- Customer Service (`inb_a3xxy`)")
st.sidebar.write("- Chat Airbnb â€“ New (`inb_d2uom`)")
st.sidebar.write("- Chat Booking â€“ New (`inb_d2xee`)")

# â€” Dozwolone ID agentÃ³w â€”
ALLOWED_IDS = {
    "tea_a2k46","tea_cj1ue","tea_cocnq","tea_cs6hi","tea_gs47r",
    "tea_h7x3r","tea_hjadz","tea_hm6zb","tea_hn7h3","tea_hn7iv",
    "tea_hnytz","tea_hnyvr","tea_97fh2"
}

# zakres czasowy: ostatnie 7 dni
now = datetime.utcnow()
seven_days_ago = now - timedelta(days=7)

@st.cache_data(ttl=300)
def fetch_and_filter(token, inbox_ids, since_ts):
    headers = {"Authorization":f"Bearer {token}","Accept":"application/json"}
    base = "https://api2.frontapp.com/conversations"
    rows = []
    total_inboxes = len(inbox_ids)
    for idx, inbox in enumerate(inbox_ids, start=1):
        params = {"inbox_id":inbox,"page_size":100}
        while True:
            resp = requests.get(base, headers=headers, params=params)
            resp.raise_for_status()
            js = resp.json()
            for conv in js.get("_results", []):
                cid = conv["id"]
                r2 = requests.get(f"{base}/{cid}/messages", headers=headers)
                r2.raise_for_status()
                for m in r2.json().get("_results", []):
                    # 1) tylko outbound
                    if m.get("is_inbound", True):
                        continue
                    # 2) data utworzenia
                    created = m.get("created_at")
                    if not created:
                        continue
                    created_dt = datetime.fromisoformat(created.replace("Z","+00:00"))
                    if created_dt < since_ts:
                        continue
                    # 3) author_id i filtr
                    raw = m.get("author") or {}
                    author_id = raw.get("id") if isinstance(raw, dict) else None
                    if author_id not in ALLOWED_IDS:
                        continue
                    # 4) strip HTML
                    text = BeautifulSoup(m.get("body",""),"html.parser").get_text("\n")
                    # 5) czytelny Author
                    if isinstance(raw, dict):
                        name   = (raw.get("first_name","") + " " + raw.get("last_name","")).strip()
                        handle = raw.get("username") or raw.get("handle") or ""
                        author = f"{name} ({handle})" if handle else name
                    else:
                        author = str(raw)
                    rows.append({
                        "Inbox ID":        inbox,
                        "Created At":      created_dt,
                        "Conversation ID": cid,
                        "Message ID":      m.get("id",""),
                        "Author ID":       author_id,
                        "Author":          author,
                        "Extract":         text
                    })
            cursor = js.get("_cursor")
            if not cursor:
                break
            params["cursor"] = cursor
        # aktualizacja paska postÄ™pu po kaÅ¼dym inboxie
        st.session_state._fetch_progress.progress(idx/total_inboxes)
    return pd.DataFrame(rows)

# inicjalizacja paska postÄ™pu do pobierania
if "_fetch_progress" not in st.session_state:
    st.session_state._fetch_progress = st.progress(0.0)

if st.button("â–¶ï¸ Pobierz i analizuj (ostatnie 7 dni)"):
    # krok 1: pobieranie + filtracja
    df = fetch_and_filter(front_token, INBOX_IDS, seven_days_ago)
    if df.empty:
        st.warning("â— Brak outbound-owych wiadomoÅ›ci od wybranych agentÃ³w w ostatnich 7 dniach.")
        st.stop()
    st.success(f"Pobrano {len(df)} wiadomoÅ›ci z ostatnich 7 dni.")
    st.dataframe(df[["Created At","Author","Extract"]].head(10), use_container_width=True)

    # krok 2: analiza przez GPT
    API_URL = "https://api.openai.com/v1/chat/completions"
    HEADERS = {"Authorization":f"Bearer {openai_key}","Content-Type":"application/json"}
    SYSTEM = (
        "JesteÅ› MenedÅ¼erem CS w Bookinghost i oceniasz jakoÅ›Ä‡ agentÃ³w "
        "w skali 1â€“5 (empatia, poprawnoÅ›Ä‡, procedury, ton). "
        "Odpowiedz formatem:\nOcena: X/5\nUzasadnienie: â€¢ pkt1\nâ€¢ pkt2"
    )

    async def analyze_one(sess, rec):
        payload = {
            "model":"gpt-3.5-turbo",
            "messages":[{"role":"system","content":SYSTEM},
                        {"role":"user","content":rec["Extract"]}],
            "temperature":0.3,"max_tokens":200
        }
        async with sess.post(API_URL, headers=HEADERS, json=payload) as r:
            js = await r.json()
        if js.get("error"):
            return f"âŒ {js['error']['message']}"
        ch = js.get("choices") or []
        if not ch: return "âŒ no choices"
        return ch[0]["message"]["content"].strip()

    async def run_all(recs, prog, stat):
        out=[]; batch=20
        async with aiohttp.ClientSession() as sess:
            total=len(recs)
            for i in range(0,total,batch):
                chunk=recs[i:i+batch]
                res=await asyncio.gather(*[analyze_one(sess,r) for r in chunk])
                out.extend(res)
                done=min(i+batch,total)
                prog.progress(done/total)
                stat.text(f"Przetworzono {done}/{total}")
        return out

    recs = df.to_dict("records")
    prog = st.progress(0.0); stat = st.empty(); start=time.time()
    with st.spinner("âš™ï¸ Analizaâ€¦"):
        df["Feedback"] = asyncio.run(run_all(recs, prog, stat))
    st.success(f"âœ… Analiza zakoÅ„czona w {time.time()-start:.1f}s")

    # krok 3: parsowanie ocen i raport
    def parse_score(t):
        for l in t.splitlines():
            if l.lower().startswith("ocena"):
                try: return float(l.split(":")[1].split("/")[0])
                except: pass
        return None

    df["Score"] = df["Feedback"].map(parse_score)
    st.header("ðŸ“ˆ Podsumowanie")
    st.metric("Åšrednia ocena", f"{df['Score'].mean():.2f}/5")
    st.metric("Liczba wiadomoÅ›ci", len(df))

    st.header("ðŸ‘¤ Raport agentÃ³w")
    rb = df.groupby("Author").agg(Åšrednia=("Score","mean"),Liczba=("Score","count")).round(2).reset_index()
    st.dataframe(rb, use_container_width=True)

    st.header("ðŸ“¥ Pobierz CSV")
    csv = df.to_csv(index=False,sep=";").encode("utf-8")
    st.download_button("â¬‡ CSV", csv, "report.csv","text/csv")
